# ======================================================
# üì∞ ui/news_tab.py ‚Äî Tab tin t·ª©c t·ª´ nhi·ªÅu ngu·ªìn
# ======================================================
import streamlit as st
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import feedparser
import json
import time
import math
import numbers
import re
from email.utils import parsedate_to_datetime

VN_STOCK_KEYWORDS = [
    "ch·ª©ng kho√°n",
    "th·ªã tr∆∞·ªùng vi·ªát nam",
    "th·ªã tr∆∞·ªùng ch·ª©ng kho√°n",
    "vn-index",
    "vnindex",
    "vn30",
    "vni",
    "hose",
    "hnx",
    "upcom",
    "vietstock",
    "doanh nghi·ªáp ni√™m y·∫øt",
    "c·ªï phi·∫øu",
    "ssi",
    "vcb",
    "vic",
    "vnm"
]

EXCLUDED_TOPIC_KEYWORDS = [
    "crypto",
    "bitcoin",
    "ethereum",
    "blockchain",
    "forex",
    "fed",
    "nasdaq",
    "dow jones",
    "s&p",
    "us market",
    "wall street",
    "goldman sachs",
    "ch·ª©ng kho√°n m·ªπ",
    "tr√°i phi·∫øu m·ªπ",
    "ti·ªÅn ·∫£o",
    "ti·ªÅn ƒëi·ªán t·ª≠"
]

VIETSTOCK_RSS_FEEDS = [
    "https://vietstock.vn/830/chung-khoan/co-phieu.rss",
    "https://vietstock.vn/739/chung-khoan/giao-dich-noi-bo.rss",
    "https://vietstock.vn/741/chung-khoan/niem-yet.rss"
]

VNECONOMY_ARTICLE_SLUG = re.compile(r"^/[\w\-/]+-e\d+\.htm$")
POSITIVE_NEWS_KEYWORDS = ["tƒÉng", "h·ªìi ph·ª•c", "l√£i", "tang", "hoi phuc", "lai"]
NEGATIVE_NEWS_KEYWORDS = ["gi·∫£m", "b√°n th√°o", "l·ªó", "giam", "ban thao", "lo"]

# ======================================================
# üîß H√ÄM PH·ª§ TR·ª¢
# ======================================================
def convert_relative_date(relative_date):
    """Chuy·ªÉn ƒë·ªïi th·ªùi gian t∆∞∆°ng ƒë·ªëi th√†nh th·ªùi gian th·ª±c"""
    try:
        if "minute" in relative_date:
            minutes = int(relative_date.split()[0])
            return datetime.now() - timedelta(minutes=minutes)
        elif "hour" in relative_date:
            hours = int(relative_date.split()[0])
            return datetime.now() - timedelta(hours=hours)
        elif "day" in relative_date:
            days = int(relative_date.split()[0])
            return datetime.now() - timedelta(days=days)
        else:
            return datetime.now()
    except Exception as e:
        st.warning(f"Error parsing date: {e}")
        return datetime.now()


def is_vietnam_stock_article(title: str, content: str) -> bool:
    """Ki·ªÉm tra b√†i vi·∫øt c√≥ li√™n quan ƒë·∫øn th·ªã tr∆∞·ªùng ch·ª©ng kho√°n Vi·ªát Nam."""
    combined_text = f"{title or ''} {content or ''}".lower()
    if any(excluded in combined_text for excluded in EXCLUDED_TOPIC_KEYWORDS):
        return False
    return any(keyword in combined_text for keyword in VN_STOCK_KEYWORDS)


def format_display_date(date_value):
    """ƒê·ªãnh d·∫°ng th·ªùi gian th√†nh chu·ªói th√¢n thi·ªán DD/MM/YYYY - HH:MM"""
    try:
        if isinstance(date_value, datetime):
            dt = date_value
        elif isinstance(date_value, numbers.Number):
            timestamp = float(date_value)
            if timestamp > 1e12:
                timestamp /= 1000  # vnstock tr·∫£ v·ªÅ millisecond
            dt = datetime.fromtimestamp(timestamp)
        elif isinstance(date_value, time.struct_time):
            dt = datetime.fromtimestamp(time.mktime(date_value))
        elif isinstance(date_value, str):
            stripped_value = date_value.strip()
            if stripped_value.isdigit():
                timestamp = float(stripped_value)
                if timestamp > 1e12:
                    timestamp /= 1000
                dt = datetime.fromtimestamp(timestamp)
            else:
                dt = parsedate_to_datetime(stripped_value)
        else:
            dt = datetime.now()

        if dt.tzinfo is not None:
            dt = dt.astimezone().replace(tzinfo=None)

        return dt.strftime("%d/%m/%Y - %H:%M")
    except Exception:
        if isinstance(date_value, str) and date_value:
            return date_value
        return datetime.now().strftime("%d/%m/%Y - %H:%M")


def get_news_sentiment_styles(title: str, content: str):
    """Determine sentiment style configuration based on simple keyword scan."""
    text = f"{title or ''} {content or ''}".lower()
    sentiment = "neutral"

    if any(keyword in text for keyword in POSITIVE_NEWS_KEYWORDS):
        sentiment = "positive"
    elif any(keyword in text for keyword in NEGATIVE_NEWS_KEYWORDS):
        sentiment = "negative"

    styles = {
        "positive": {
            "border": "#22c55e",
            "background": "linear-gradient(135deg, #f0fdf4 0%, #dcfce7 100%)",
            "label": "Tin t√≠ch c·ª±c"
        },
        "negative": {
            "border": "#ef4444",
            "background": "linear-gradient(135deg, #fee2e2 0%, #fecaca 100%)",
            "label": "Tin ti√™u c·ª±c"
        },
        "neutral": {
            "border": "#d97706",
            "background": "linear-gradient(135deg, #fef3c7 0%, #fde68a 100%)",
            "label": "Tin trung l·∫≠p"
        }
    }
    return styles[sentiment]


@st.cache_data(ttl=300, show_spinner=False)
def fetch_rss_news(source="vnexpress", max_articles=5):
    """L·∫•y tin t·ª´ RSS Feed - Ph∆∞∆°ng ph√°p ƒë√°ng tin c·∫≠y h∆°n"""
    
    # Special handling for vnEconomy - use web scraping instead
    if source == "vnEconomy":
        return scrape_vneconomy_news(max_articles)
    
    rss_urls = {
        "vnexpress": "https://vnexpress.net/rss/kinh-doanh.rss",
        "cafef": "https://cafef.vn/thi-truong-chung-khoan.rss",
        "vietstock": VIETSTOCK_RSS_FEEDS
    }
    
    if source not in rss_urls:
        return []
    
    urls = rss_urls[source]
    if not isinstance(urls, list):
        urls = [urls]

    aggregated_news = []
    last_warning = None

    # Try each URL and accumulate until we have enough articles
    for url_index, url in enumerate(urls):
        try:
            # Enhanced headers to avoid blocking
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'application/rss+xml, application/xml, text/xml, */*',
                'Accept-Language': 'vi-VN,vi;q=0.9,en-US;q=0.8,en;q=0.7',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
                'Cache-Control': 'no-cache',
                'Pragma': 'no-cache'
            }
            
            # D√πng requests ƒë·ªÉ l·∫•y RSS v·ªõi timeout ng·∫Øn
            response = requests.get(url, headers=headers, timeout=15, allow_redirects=True)
            response.raise_for_status()
            
            # Parse RSS
            feed = feedparser.parse(response.content)
            
            # Check if feed has entries
            if not feed.entries:
                last_warning = f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y b√†i vi·∫øt t·ª´ {source}"
                continue
            
            for entry in feed.entries:
                if len(aggregated_news) >= max_articles:
                    break
                try:
                    title = entry.title if hasattr(entry, 'title') else "No Title"
                    link = entry.link if hasattr(entry, 'link') else ""
                    
                    # Parse date
                    published_struct = getattr(entry, 'published_parsed', None)
                    updated_struct = getattr(entry, 'updated_parsed', None)
                    if published_struct:
                        date = format_display_date(published_struct)
                    elif updated_struct:
                        date = format_display_date(updated_struct)
                    elif hasattr(entry, 'published'):
                        date = format_display_date(entry.published)
                    elif hasattr(entry, 'updated'):
                        date = format_display_date(entry.updated)
                    else:
                        date = format_display_date(datetime.now())
                    
                    # Get content
                    content = ""
                    if hasattr(entry, 'summary'):
                        content = BeautifulSoup(entry.summary, 'html.parser').get_text(strip=True)
                    elif hasattr(entry, 'description'):
                        content = BeautifulSoup(entry.description, 'html.parser').get_text(strip=True)
                    else:
                        content = "N·ªôi dung ƒëang ƒë∆∞·ª£c c·∫≠p nh·∫≠t..."
                    
                    normalized_content = content[:500] + "..." if len(content) > 500 else content
                    if not is_vietnam_stock_article(title, normalized_content):
                        continue

                    aggregated_news.append({
                        "title": title,
                        "date": date,
                        "content": normalized_content,
                        "link": link,
                        "source": source.upper()
                    })
                except Exception:
                    continue
            
            if len(aggregated_news) >= max_articles:
                break
                
        except requests.exceptions.HTTPError as e:
            error_msg = f"HTTP {e.response.status_code}"
            last_warning = f"‚ö†Ô∏è Kh√¥ng th·ªÉ t·∫£i RSS t·ª´ {source}: {error_msg}"
            continue
        except requests.exceptions.Timeout:
            last_warning = f"‚ö†Ô∏è Timeout khi t·∫£i RSS t·ª´ {source}"
            continue
        except requests.exceptions.ConnectionError:
            last_warning = f"‚ö†Ô∏è L·ªói k·∫øt n·ªëi ƒë·∫øn {source}"
            continue
        except Exception as e:
            last_warning = f"‚ö†Ô∏è Kh√¥ng th·ªÉ t·∫£i RSS t·ª´ {source}: {str(e)[:80]}"
            continue

    if aggregated_news:
        return aggregated_news[:max_articles]

    if last_warning:
        st.warning(last_warning)
    else:
        st.warning(f"‚ö†Ô∏è Kh√¥ng th·ªÉ t·∫£i RSS t·ª´ {source}")
    return []


@st.cache_data(ttl=300, show_spinner=False)
def scrape_vneconomy_news(max_articles=5):
    """
    Web scraping cho vnEconomy khi RSS kh√¥ng ho·∫°t ƒë·ªông
    """
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'vi-VN,vi;q=0.9,en-US;q=0.8,en;q=0.7',
            'Connection': 'keep-alive'
        }
        
        base_section = "https://vneconomy.vn/chung-khoan.htm"
        max_section_pages = 5  # crawl deeper pages to get ƒë·ªß b√†i li√™n quan ch·ª©ng kho√°n
        urls_to_try = []

        for page in range(1, max_section_pages + 1):
            if page == 1:
                urls_to_try.append(base_section)
            else:
                urls_to_try.append(f"{base_section}?p={page}")

        # Fallback pages b·ªï sung th√™m b·ªëi c·∫£nh kinh t·∫ø Vi·ªát Nam n·∫øu trang ch√≠nh thi·∫øu b√†i
        urls_to_try.extend([
            "https://vneconomy.vn/kinh-te.htm",
            "https://vneconomy.vn"
        ])
        
        collected_news = []
        seen_links = set()

        for base_url in urls_to_try:
            try:
                response = requests.get(base_url, headers=headers, timeout=15)
                response.raise_for_status()
                soup = BeautifulSoup(response.content, 'html.parser')
                
                page_news = []
                
                # Find article containers - vnEconomy uses different classes
                # Try multiple possible selectors
                article_selectors = [
                    'div.story',
                    'div.story-item',
                    'article.story',
                    'div.news-item',
                    'div.item-news'
                ]
                
                articles = []
                for selector in article_selectors:
                    articles = soup.select(selector)
                    if articles:
                        break
                
                if not articles:
                    # Fallback: find any links that look like articles
                    articles = soup.find_all('a', href=True)
                    articles = [a for a in articles if '/tin-tuc/' in a.get('href', '') or '/kinh-te/' in a.get('href', '')][:max_articles * 2]
                
                for article in articles[:max_articles * 3]:
                    if len(collected_news) >= max_articles:
                        break
                    
                    try:
                        # Extract title
                        title_elem = article.find('h3') or article.find('h2') or article.find('a')
                        if not title_elem:
                            continue
                        
                        title = title_elem.get_text(strip=True)
                        if not title or len(title) < 10:
                            continue
                        
                        # Extract link
                        link_elem = article.find('a') if article.name != 'a' else article
                        link = link_elem.get('href', '') if link_elem else ''
                        if link and not link.startswith('http'):
                            link = f"https://vneconomy.vn{link}"
                        
                        # Extract date
                        time_elem = article.find('time') or article.find('span', class_=['time', 'date', 'published'])
                        raw_date = time_elem.get_text(strip=True) if time_elem else datetime.now()
                        date = format_display_date(raw_date) if raw_date else format_display_date(datetime.now())
                        
                        # Extract description
                        desc_elem = article.find('p') or article.find('div', class_=['description', 'desc', 'summary'])
                        content = desc_elem.get_text(strip=True) if desc_elem else "ƒê·ªçc th√™m t·∫°i vneconomy.vn"
                        
                        if len(content) < 20:
                            content = f"{title[:100]}... ƒê·ªçc th√™m t·∫°i vneconomy.vn"
                        
                        normalized_content = content[:500] + "..." if len(content) > 500 else content

                        passes_filter = is_vietnam_stock_article(title, normalized_content)
                        lower_text = f"{title} {normalized_content}".lower()
                        if not passes_filter:
                            if (link.startswith("https://vneconomy.vn/chung-khoan") or link.startswith("/chung-khoan") or "chung-khoan" in base_url.lower()) and not any(excluded in lower_text for excluded in EXCLUDED_TOPIC_KEYWORDS):
                                passes_filter = True
                        if not passes_filter:
                            continue

                        unique_key = link or title
                        if unique_key in seen_links:
                            continue
                        seen_links.add(unique_key)

                        page_news.append({
                            "title": title,
                            "date": date,
                            "content": normalized_content,
                            "link": link,
                            "source": "VNECONOMY "
                        })
                    except Exception:
                        continue
                
                if len(collected_news) + len(page_news) < max_articles:
                    for anchor in soup.find_all('a', href=True):
                        if len(collected_news) + len(page_news) >= max_articles:
                            break
                        raw_href = anchor.get('href', '')
                        if not raw_href or raw_href.startswith('javascript') or raw_href.startswith('#'):
                            continue
                        if not VNECONOMY_ARTICLE_SLUG.match(raw_href):
                            continue
                        anchor_title = anchor.get_text(strip=True)
                        if not anchor_title or len(anchor_title) < 10:
                            continue
                        link = raw_href if raw_href.startswith('http') else f"https://vneconomy.vn{raw_href}"
                        if link in seen_links:
                            continue

                        placeholder_content = f"Tin nhanh VnEconomy: {anchor_title}. ƒê·ªçc n·ªôi dung chi ti·∫øt tr√™n trang g·ªëc."
                        passes_filter = is_vietnam_stock_article(anchor_title, placeholder_content)
                        if not passes_filter:
                            lower_text = anchor_title.lower()
                            if (link.startswith("https://vneconomy.vn/chung-khoan") or raw_href.startswith("/chung-khoan") or "chung-khoan" in base_url.lower()) and not any(excluded in lower_text for excluded in EXCLUDED_TOPIC_KEYWORDS):
                                passes_filter = True
                        if not passes_filter:
                            continue

                        seen_links.add(link)
                        page_news.append({
                            "title": anchor_title,
                            "date": format_display_date(datetime.now()),
                            "content": placeholder_content,
                            "link": link,
                            "source": "VNECONOMY "
                        })

                if page_news:
                    collected_news.extend(page_news)
                    if len(collected_news) >= max_articles:
                        return collected_news[:max_articles]
                    
            except Exception:
                continue
        
        return collected_news
        
    except Exception as e:
        st.warning(f"‚ö†Ô∏è Kh√¥ng th·ªÉ scrape vnEconomy: {str(e)[:80]}")
        return []


@st.cache_data(ttl=300, show_spinner=False)  # Cache 5 ph√∫t
def scrape_investing_news(page_num, max_articles=5):
    """
    Scrape tin t·ª©c t·ª´ Investing.com
    
    Args:
        page_num: S·ªë trang c·∫ßn crawl
        max_articles: S·ªë b√†i vi·∫øt t·ªëi ƒëa c·∫ßn l·∫•y
    
    Returns:
        List[dict]: Danh s√°ch tin t·ª©c
    """
    # URL ƒë√∫ng cho Investing.com stock market news
    if page_num == 1:
        url = "https://www.investing.com/news/stock-market-news"
    else:
        url = f"https://www.investing.com/news/stock-market-news/{page_num}"
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.5",
        "Accept-Encoding": "gzip, deflate, br",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
        "Sec-Fetch-Dest": "document",
        "Sec-Fetch-Mode": "navigate",
        "Sec-Fetch-Site": "none",
        "Cache-Control": "max-age=0"
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=15, verify=True)
        response.raise_for_status()
    except requests.exceptions.RequestException as e:
        st.error(f"‚ö†Ô∏è Kh√¥ng th·ªÉ k·∫øt n·ªëi ƒë·∫øn Investing.com: {str(e)[:100]}")
        st.info("üí° C√≥ th·ªÉ do: (1) M·∫°ng b·ªã ch·∫∑n, (2) Website ƒëang b·∫£o tr√¨, (3) C·∫ßn VPN")
        return []

    soup = BeautifulSoup(response.content, 'html.parser')
    articles = soup.find_all('div', class_='news-analysis-v2_content__z0iLP w-full text-xs sm:flex-1')

    news_data = []
    for article in articles:
        if len(news_data) >= max_articles:
            break
            
        try:
            # L·∫•y ti√™u ƒë·ªÅ
            title_elem = article.find(
                'a',
                class_='text-inv-blue-500 hover:text-inv-blue-500 hover:underline focus:text-inv-blue-500 focus:underline whitespace-normal text-sm font-bold leading-5 !text-[#181C21] sm:text-base sm:leading-6 lg:text-lg lg:leading-7'
            )
            if not title_elem:
                continue
            title = title_elem.get_text(strip=True)

            # L·∫•y th·ªùi gian
            time_elem = article.find('time')
            if time_elem:
                date_text = time_elem.get_text(strip=True)
                if "ago" in date_text:
                    date = format_display_date(convert_relative_date(date_text))
                else:
                    date = format_display_date(date_text)
            else:
                date = format_display_date(datetime.now())

            # L·∫•y li√™n k·∫øt b√†i vi·∫øt chi ti·∫øt
            link = title_elem.get('href', '')
            if link.startswith("http"):
                full_link = link
            else:
                full_link = f"https://www.investing.com{link}"

            # L·∫•y n·ªôi dung b√†i vi·∫øt chi ti·∫øt
            content = "Loading..."
            try:
                detail_response = requests.get(full_link, headers=headers, timeout=10)
                detail_response.raise_for_status()
                detail_soup = BeautifulSoup(detail_response.content, 'html.parser')
                content_div = detail_soup.find('div', class_='article_WYSIWYG__O0uhw article_articlePage__UMz3q text-[18px] leading-8')
                content = content_div.get_text(strip=True) if content_div else "No Content Available"
            except requests.exceptions.RequestException as e:
                content = f"Error retrieving content: {e}"

            if not is_vietnam_stock_article(title, content):
                continue

            news_data.append({
                "title": title,
                "date": date,
                "content": content,
                "link": full_link
            })
            
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Error processing article: {e}")
            continue

    return news_data


def render_pagination_controls(total_pages):
    """Hi·ªÉn th·ªã ƒëi·ªÅu h∆∞·ªõng trang ·ªü cu·ªëi tab"""
    st.divider()
    spacer_left, control_col, spacer_right = st.columns([1, 2, 1])

    with control_col:
        prev_col, info_col, next_col = st.columns([1, 1, 1], gap="small")

        prev_disabled = st.session_state.news_current_page <= 1
        next_disabled = st.session_state.news_current_page >= total_pages

        if prev_col.button("‚¨ÖÔ∏è", use_container_width=True, disabled=prev_disabled, key="news_prev_btn"):
            st.session_state.news_current_page -= 1
            st.rerun()

        info_col.markdown(
            f"<div style='text-align:center; font-size:16px; font-weight:600;'>Trang {st.session_state.news_current_page} / {total_pages}</div>",
            unsafe_allow_html=True
        )

        if next_col.button("‚û°Ô∏è", use_container_width=True, disabled=next_disabled, key="news_next_btn"):
            st.session_state.news_current_page += 1
            st.rerun()


# ======================================================
# üì∞ RENDER TAB NEWS
# ======================================================
def render(ticker: str = None):
    """Hi·ªÉn th·ªã tab tin t·ª©c t·ª´ nhi·ªÅu ngu·ªìn"""
    
    st.header("üì∞ Tin t·ª©c Th·ªã tr∆∞·ªùng Ch·ª©ng kho√°n Vi·ªát Nam")
    
    # Ch·ªçn ngu·ªìn tin
    col1, col2 = st.columns([3, 1])
    with col1:
        st.markdown("""
        <p style='color:#94a3b8'>
        Tin t·ª©c m·ªõi nh·∫•t v·ªÅ th·ªã tr∆∞·ªùng ch·ª©ng kho√°n Vi·ªát Nam t·ª´ nhi·ªÅu ngu·ªìn tin uy t√≠n.
        </p>
        """, unsafe_allow_html=True)
    
    with col2:
        news_source = st.selectbox(
            "üì° Ch·ªçn ngu·ªìn:",
            ["vnexpress", "cafef", "vietstock", "vnEconomy"],
            format_func=lambda x: {
                "vnexpress": "VnExpress",
                "cafef": "CafeF", 
                "vietstock": "VietStock",
                "vnEconomy": "VnEconomy"
            }.get(x, x)
        )
    
    # Kh·ªüi t·∫°o session state cho s·ªë trang
    if 'news_current_page' not in st.session_state:
        st.session_state.news_current_page = 1
    
    per_page = 5
    
    # ======================================================
    # üìä L·∫§Y V√Ä HI·ªÇN TH·ªä TIN T·ª®C
    # ======================================================
    # L·∫•y nhi·ªÅu tin t·ª©c ƒë·ªÉ ph√¢n trang
    with st.spinner(f"üîç ƒêang t·∫£i tin t·ª©c t·ª´ {news_source.upper()}..."):
        news = fetch_rss_news(news_source, max_articles=50)
    
    if not news:
        st.error(f"‚ùå Kh√¥ng th·ªÉ t·∫£i tin t·ª©c t·ª´ ngu·ªìn {news_source.upper()}")
        
        # Hi·ªÉn th·ªã h∆∞·ªõng d·∫´n kh·∫Øc ph·ª•c
        st.markdown("""
        ### üîß Nguy√™n nh√¢n c√≥ th·ªÉ:
        
        1. **üåê K·∫øt n·ªëi m·∫°ng**: Ki·ªÉm tra internet c·ªßa b·∫°n
        2. **üö´ Website ch·∫∑n**: Ngu·ªìn tin c√≥ th·ªÉ ch·∫∑n request t·ª± ƒë·ªông
        3. **üîí Firewall/Antivirus**: C√≥ th·ªÉ ƒëang ch·∫∑n k·∫øt n·ªëi
        4. **‚è±Ô∏è Timeout**: Server ph·∫£n h·ªìi qu√° ch·∫≠m
        
        ### üí° Gi·∫£i ph√°p:
        
        - **Th·ª≠ ngu·ªìn kh√°c**: Ch·ªçn ngu·ªìn tin kh√°c trong dropdown ·ªü tr√™n
        - Refresh l·∫°i trang sau v√†i gi√¢y
        - Ki·ªÉm tra k·∫øt n·ªëi internet
        """)
        
        return  # D·ª´ng execution n·∫øu kh√¥ng c√≥ tin t·ª©c
    else:
        total_pages = max(1, math.ceil(len(news) / per_page))
        current_page = min(st.session_state.news_current_page, total_pages)
        if current_page != st.session_state.news_current_page:
            st.session_state.news_current_page = current_page
            st.rerun()
        start_idx = (current_page - 1) * per_page
        page_news = news[start_idx:start_idx + per_page]
        if not page_news and current_page > 1:
            st.session_state.news_current_page = 1
            st.rerun()

        # Hi·ªÉn th·ªã t·ª´ng b√†i vi·∫øt
        for index, item in enumerate(page_news, start=start_idx + 1):
            sentiment_styles = get_news_sentiment_styles(item['title'], item['content'])
            border_color = sentiment_styles['border']
            background_style = sentiment_styles['background']
            sentiment_label = sentiment_styles['label']
            title_link = f"<a href='{item['link']}' target='_blank' style='color:#0f172a; text-decoration:none;'>{item['title']}</a>"

            with st.container():
                st.markdown(f"""
                <div style='
                    background: {background_style};
                    border-left: 4px solid {border_color};
                    padding: 15px;
                    border-radius: 8px;
                    margin-bottom: 15px;
                '>
                    <div style='display: flex; justify-content: space-between; align-items: center; gap: 16px;'>
                        <h4 style='color: #0f172a; margin: 0 0 10px 0; flex: 1;'>üì∞ {title_link}</h4>
                        <span style='font-size:12px; font-weight:600; color:{border_color}; padding:4px 10px; border:1px solid {border_color}; border-radius:999px;'>
                            {sentiment_label}
                        </span>
                    </div>
                    <p style='color: #6b7280; font-size: 14px; margin: 0;'>
                        üìÖ <b>ƒêƒÉng l√∫c:</b> {item['date']}
                    </p>
                </div>
                """, unsafe_allow_html=True)

                st.write(item['content'])
                st.markdown("<br>", unsafe_allow_html=True)
    
    render_pagination_controls(total_pages if 'total_pages' in locals() else 1)
